{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e5346b",
   "metadata": {},
   "source": [
    "## MNIST Basic Approach (Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "19579e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "tf.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "706ec8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f29abfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize and reshape features\n",
    "x_train = (x_train/255).reshape(x_train.shape[0], 784)\n",
    "x_test = (x_test/255).reshape(x_test.shape[0], 784)\n",
    "\n",
    "# one-hot encoding of labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10, dtype=int)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8c571245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c008b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7482d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Operations\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "64d41e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e3a1c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5d9fc94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9176\n"
     ]
    }
   ],
   "source": [
    "# Session\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    batch_size = 100\n",
    "    max_index = x_train.shape[0]\n",
    "    for step in range(1000):\n",
    "        start = np.random.randint(0, max_index - batch_size)\n",
    "        end = start + batch_size\n",
    "        \n",
    "        session.run(train, feed_dict={x: x_train[start:end], y_true: y_train[start:end]})\n",
    "    \n",
    "    # Evaluate\n",
    "    correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(y_true, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    print(session.run(acc, feed_dict={x: x_test, y_true: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480264dc",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "- Just like the simple perceptron, CNNs also have their origins in biological research\n",
    "- Hubel and Wisel studied the structure of the visual cortex in mammals, winning a Nobel Prize in 1981\n",
    "- Their research revealed that neurons in the visual cortex had a small local receptive field\n",
    "- This idea then inspired an ANN architecture that would become CNN\n",
    "- Famously implemented in the 1998 paper by Yann LeCun et al\n",
    "- The LeNet-5 architecture was first used to classify the MNIST data set\n",
    "\n",
    "#### Concepts\n",
    "- Tensors: N-dimensional arrays\n",
    "    - scalar -> 3\n",
    "    - vector -> [1,2]\n",
    "    - matrix -> [[1,2],[3,4],...,[8,9]]\n",
    "    - tensor -> [[[1,2],[3,4]],...,[[8,9],[10,11]]]\n",
    "    \n",
    "- Densely Connected Layer: each neuron is connected to every neuron in the next layer\n",
    "- Convolutional Layer: each unit is connected to a smaller number of nearby units in the next layer\n",
    "    - MNIST dataset is 28x28 pixels, but most images are at least 256x256 or greater - or a total of <56K\n",
    "    - This leads to too many parameters, unscalable to new images\n",
    "    - Convolutions also have a major advantage for image processing, where pixels nearby to each other are much more correlated to each other for image detection\n",
    "    - Each CNN layer looks at an increasingly larger part of the image\n",
    "    - Having units only connected to nearby units also aids in *invariance*\n",
    "    - CNN also helps with regularization, limiting the search of weights to the size of the convolution\n",
    "    - Convolution:\n",
    "        - Filters and filter size\n",
    "            - Commonly visualized with grids, where we pass the filters (grid of weights) through the input, and compute the multiplication of the weights in the filter and the input. Then sum the results to get a final output\n",
    "        - Stride\n",
    "            - We move the filter grid over by the amount of the stride - e.g. 1 or 2 pixels\n",
    "- Pooling Layer: subsample the input image, which reduces the memory use and computer load as well as reducing the number of parameters\n",
    "    - Create an N x N pool of pixels and evaluate the maximum value - only that value makes it to the next layer (representative value)\n",
    "    - Move over by the value of the stride and repeat the process\n",
    "    - This end up removing a lot of information. Even a small pooling \"kernel\" of 2x2 with a stride of 2 will remove 75% of the input data\n",
    "- Dropout: can be thought of as a form of regularization to help prevent overfitting.\n",
    "    - During training, units are randomly dropped, along with their connections\n",
    "    - This helps prevent units from \"co-adapting\" too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a9ca3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    # x --> [batch,height,width,Channels]\n",
    "    # W --> [filter height, filter width, Channels In, Channels Out]\n",
    "    \n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "def max_pooling_2by2(x):\n",
    "    # x --> [batch,height,width,Channels]\n",
    "    # Pooling along height and width only; that's why [1,2,2,1]\n",
    "    pool_along_height_width = [1,2,2,1]\n",
    "    return tf.nn.max_pool(x, ksize=pool_along_height_width, strides=pool_along_height_width, padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6ebf22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    \n",
    "    return tf.nn.relu(conv2d(input_x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "65c6fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    \n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ef8d3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dfd4b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "x_image = tf.reshape(x, [-1,28,28,1]) # putting flatten image back into normal shape 28x28\n",
    "\n",
    "convo_1 = convolutional_layer(x_image, shape=[5,5,1,32])\n",
    "convo_1_pooling = max_pooling_2by2(convo_1)\n",
    "\n",
    "convo_2 = convolutional_layer(convo_1_pooling, shape=[5,5,32,64])\n",
    "convo_2_pooling = max_pooling_2by2(convo_2)\n",
    "\n",
    "convo_2_flat = tf.reshape(convo_2_pooling, [-1,7*7*64])\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "02b7c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\marco\\.virtualenvs\\tensorflow_for_deep_learning-4C26M6Uv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Dropout\n",
    "hold_prob = tf.placeholder(tf.float32)\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one, keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fa2f6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = normal_full_layer(full_one_dropout, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1a02f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "45baf1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114a70c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m init \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mglobal_variables_initializer()\n\u001b[0;32m      3\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mSession(config\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mConfigProto(log_device_placement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;28;01mas\u001b[39;00m session:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "steps = 1000\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    for i in range(steps+1):\n",
    "        batch_size = 50\n",
    "        max_index = x_train.shape[0]\n",
    "        \n",
    "        start = np.random.randint(0, max_index - batch_size)\n",
    "        end = start + batch_size\n",
    "\n",
    "        session.run(train, feed_dict={x: x_train[start:end], y_true: y_train[start:end], hold_prob: 0.5})\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            matches = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true,1))\n",
    "            acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "            print(f\"ON STEP: {i}\")\n",
    "            print(f\"Accuracy: {session.run(acc, feed_dict={x: x_test, y_true: y_test, hold_prob: 1.0})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "38d92bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7b418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfdl",
   "language": "python",
   "name": "tfdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
